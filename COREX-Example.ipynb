{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import scipy.sparse as ss\n",
    "from corextopic import corextopic as ct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello All,\\n\\nI have a PC Transporter for sale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nI've been saying that for at least 2 years n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nNevertheless, DWI is F*ckin serious.  Hope y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nthe last arab country was syria. but not a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n ##flame thrower on## \\n Well I don't want m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Hello All,\\n\\nI have a PC Transporter for sale...\n",
       "1  \\nI've been saying that for at least 2 years n...\n",
       "2  \\nNevertheless, DWI is F*ckin serious.  Hope y...\n",
       "3  \\n\\nthe last arab country was syria. but not a...\n",
       "4  \\n ##flame thrower on## \\n Well I don't want m..."
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and Format Data\n",
    "\n",
    "\n",
    "#Categories to load from 20NewsGroups Data Set \n",
    "categories = ['comp.graphics',\n",
    "             'comp.sys.ibm.pc.hardware',\n",
    "             'misc.forsale',\n",
    "             'rec.motorcycles',\n",
    "             'rec.sport.baseball',\n",
    "             'sci.crypt',\n",
    "             'sci.electronics',\n",
    "             'sci.med',\n",
    "             'sci.space',\n",
    "             'soc.religion.christian',\n",
    "             'talk.politics.guns',\n",
    "             'talk.politics.mideast']\n",
    "\n",
    "#Load Data\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  categories = categories,\n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n",
    "\n",
    "#Convert corpus into pandas data frame\n",
    "corpus = pd.DataFrame(twenty_train.data,columns = ['text'])\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Preprocessing lists \n",
    "#Initialize N-Grams List\n",
    "ngrams_list = ['united states','new york','law enforcement','los angeles','hard disk',\n",
    "               'power play','mailing list','serialnumber','health care','window manager',\n",
    "               'email address','white house','gun control','san francisco','san jose',\n",
    "               'gordon banks','washington dc','st louis','years old','public key',\n",
    "               'source code','vocal cord','second amendment','video card','jesus christ',\n",
    "               'power supply','human rights','last night','young people','public domain',\n",
    "               'medical newsletter','world war','floppy disk','image processing',\n",
    "               'senior administration','volumn number','administration official',\n",
    "               'holy spirit','space shuttle','hockey league','bear arms','electronic mail',\n",
    "               'third person','space station','federal government',\n",
    "               'armenian government','san diego','us government','tampa bay','vice president',\n",
    "               'new testament','high speed','soviet union','private sector']\n",
    "\n",
    "#Additional corpus specific stopwords\n",
    "additional_stopwords = ['one','would','people','like','get','dont','know','also',\n",
    "                        'use','u','make','say','year','could','x','may','good','well',\n",
    "                        'im','even','new','see','way','thing','right','two','first',\n",
    "                        'much','many','want','need','go','used','said','question','anyone',\n",
    "                        'take','come','something','bit','since','using','going','back',\n",
    "                        'look','really','still','must','might','help','b','got','last',\n",
    "                        'please','ive','give','sure','cant','without','set','never',\n",
    "                        'better','another','didnt','doesnt','c','someone','etc','thats',\n",
    "                        'put','try','least','however','anything','every','second','do',\n",
    "                       'a','b','c','d','e','f','g','h','i','j','k','l','m','n'\n",
    "                        ,'o','p','q','r','s','t','u','v','w','x','y','z','let','lets']\n",
    "\n",
    "#Initialize Word Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "#Load general NLTK Stopwords\n",
    "stopwords_list = list(stopwords.words('english'))\n",
    "\n",
    "#Create single list of Stopwords\n",
    "stopwords_list.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ngrams(ngrams):\n",
    "    '''\n",
    "    Input: List of pairs of n-grams\n",
    "    Output: Dictionary of ngram pairs to single token (EX: {stop word: stop_word})\n",
    "    \n",
    "    '''\n",
    "    ngram_dict = dict({})\n",
    "    for i in ngrams:\n",
    "        ngram_dict.update({i:re.sub(' ','_',i)})\n",
    "        \n",
    "    return(ngram_dict)\n",
    "\n",
    "#Create dictionary of n-grams\n",
    "ngrams_dict = convert_ngrams(ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line,stopwords_list = stopwords_list,ngrams_dict = ngrams_dict):\n",
    "    '''\n",
    "    Input: Single document from corpus\n",
    "    Output: cleaned document \n",
    "    \n",
    "    Steps:\n",
    "        1. Removes numbers, new lines, and tabs. Brings to lowercase\n",
    "        2. Removes punctuation\n",
    "        3. Removes extra whitespace\n",
    "        4. Introduce n-grams\n",
    "        5. Remove stopwords and lemmatize\n",
    "        6. Strip whitespace\n",
    "    '''\n",
    "    #remove all numbers, tabs, and new lines\n",
    "    line = re.sub('\\d+|\\n|\\t',' ',line.lower())\n",
    "    #remove all punctuation\n",
    "    line = re.sub('([!\"#$%&\\'()*\\+,-./:;<=>\\?\\@\\[\\\\]\\^_`{|}~])', ' ', line)\n",
    "    #remove extra white space\n",
    "    line = re.sub('\\s{2,}', ' ', line)\n",
    "    #introduce n-grams\n",
    "    for word_pair in ngrams_dict:\n",
    "        line = re.sub(word_pair,ngrams_dict[word_pair],line)\n",
    "   #remove stopwords and lemmatize\n",
    "    line = ' '.join([lemmatizer.lemmatize(i) for i in line.split(' ') if i not in stopwords_list])\n",
    "    line = line.strip()\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Preprocessing\n",
    "corpus['clean_text'] = corpus['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b57b213a96145afa517d775e908cf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7036), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c1436d22d0433cbc06ad72ba1d49e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45665), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Secondary preprocessing step\n",
    "'''\n",
    "Input: Cleaned Corpus\n",
    "Output: Cleaned Corpus with only words that occur more than 5 times\n",
    "\n",
    "Purpose: removes low frequency words. Minimal information loss for major performace boost\n",
    "'''\n",
    "\n",
    "#Create list of all words in corpus\n",
    "words = list()\n",
    "for i in tqdm(corpus['clean_text']):\n",
    "    words.extend(i.split(' '))\n",
    "\n",
    "#Get frequency counts of all unique words\n",
    "counted_words = Counter(words).most_common()[::-1]\n",
    "\n",
    "#Iterate and create vocab of high frequency words (More than 5 occurances)\n",
    "vocab = list()\n",
    "for word,count in tqdm(counted_words):\n",
    "    if count > 5:\n",
    "        vocab.append(word)\n",
    "\n",
    "def low_frequency_word_removal(doc, vocab = vocab):\n",
    "    '''\n",
    "    Input: Document and vocabulary\n",
    "    Output: Document with only words in the vocabulary\n",
    "    '''\n",
    "    return(' '.join([i for i in doc.split(' ') if i in vocab]))\n",
    "\n",
    "#Apply low frequency word removal\n",
    "corpus['final_text'] = corpus['clean_text'].apply(low_frequency_word_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize vectorizer, no n-grams (We already did this earlier)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "#Vectorize corpus of cleaned data\n",
    "X = vectorizer.fit_transform(corpus['final_text'] ).toarray()\n",
    "\n",
    "#Gets the vocabulary of the corpus\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "#Convert to Spacy Matrix for input into topic model \n",
    "X = ss.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize anchor words\n",
    "#create a dictionary of category to topic words\n",
    "\n",
    "anchor_topic_dict = dict({\n",
    "                            'comp.graphics':['image','file','graphic','jpeg','format','color','system'\n",
    "                                             ,'software','data','version','gif','program'],\n",
    "    \n",
    "                            'comp.sys.ibm.pc.hardware':['drive','scsi','card','system','mb','controller',\n",
    "                                                        'problem','bus','ide','pc','work','driver','disk'],\n",
    "    \n",
    "                            'misc.forsale':['sale','offer','price','shipping','game','condition','drive'\n",
    "                                            ,'sell','manual','mail'],\n",
    "    \n",
    "                            'rec.motorcycles':['bike','dod','motorcycle','ride','time','helmet','rider',\n",
    "                                               'road','cop','court','driver','ticket','lawyer','speeding','vehicle'],\n",
    "    \n",
    "                            'rec.sport.baseball':['game','team','run','player','hit','baseball','time','win',\n",
    "                                                  'league','season','cub','play','pitching','pitcher'],\n",
    "    \n",
    "                            'sci.crypt':['key','encryption','db','chip','system','privacy','security',\n",
    "                                         'information','message','algorithm','number','data','file'], \n",
    "    \n",
    "                            'sci.electronics':['wire','circuit','ground','power','work','current',\n",
    "                                             'line','wiring','voltage','amp','outlet','time','chip',\n",
    "                                             'number','data','system','radar','mhz','work','number',\n",
    "                                               'time','information'],\n",
    "    \n",
    "                            'sci.med':['patient','time','disease','food','health','doctor','problem',\n",
    "                                       'study','medical','pain','cancer','research','information'], \n",
    "    \n",
    "                            'sci.space':['space','nasa','launch','satellite','system','time','orbit',\n",
    "                                         'mission','earth','data','program','lunar','moon','rocket'], \n",
    "    \n",
    "                            'soc.religion.christian':['christian','jesus','church','think',\n",
    "                                                      'time','believe','faith','bible','christ','life',\n",
    "                                                      'truth','belief','law','scripture','father',\n",
    "                                                      'hell','son','love','paul','catholic','christianity'], \n",
    "    \n",
    "                            'talk.politics.guns':['government','criminal','handgun','bill','fire','police',\n",
    "                                                  'defence','case','militia','gun_control'], \n",
    "    \n",
    "                            'talk.politics.mideast':['armenian','israel','turkish','jew','israeli',\n",
    "                                                     'time','arab','turkey','greek','turk','state','muslim',\n",
    "                                                     'armenia','woman','government','killed','jewish',\n",
    "                                                     'village','azerbaijani','russian','soldier','palestinian',\n",
    "                                                     'country']\n",
    "                           })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of list of topics + words, also create mapping of topic to topic number\n",
    "anchor_topic_numbers = dict({})\n",
    "num_to_topic = dict({})\n",
    "anchor_topics = list()\n",
    "for i,topic in enumerate(anchor_topic_dict):\n",
    "    anchor_topic_numbers.update({topic:i})\n",
    "    num_to_topic.update({i:topic})\n",
    "    anchor_topics.append(anchor_topic_dict[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09a9ae0e85d4988bc50c7bd344381d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run Topic Model\n",
    "#Number of topics\n",
    "n_Tot_clusters = 12\n",
    "\n",
    "#COREX is highly dependent on initialization, repeat 4 times and average results \n",
    "for i in tqdm(range(4)):\n",
    "    #Initialize topic model \n",
    "    topic_model = ct.Corex(n_hidden=n_Tot_clusters)  \n",
    "    #Fit topic model \n",
    "    topic_model.fit(X, words=vocab, anchors=anchor_topics, anchor_strength=3)\n",
    "    #Store results\n",
    "    if i == 0:\n",
    "        max_topic_temp = topic_model.p_y_given_x\n",
    "    else:\n",
    "        max_topic_temp += topic_model.p_y_given_x\n",
    "#Average Results\n",
    "max_topic_temp = max_topic_temp/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topic assignment \n",
    "max_topic = np.argmax(max_topic_temp,axis = 1)\n",
    "\n",
    "#If the model is completely, uncertain, we assign to noise\n",
    "noise_index = np.where(np.max(max_topic_temp,axis = 1) == 0.0)\n",
    "\n",
    "#Add topic assignments to documents\n",
    "corpus['topic_guess'] = max_topic\n",
    "corpus['topic_guess'].loc[noise_index] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_number(assignment,anchor_topic_numbers = anchor_topic_numbers):\n",
    "    '''\n",
    "    Input: Topic Assignment\n",
    "    Output: Topic Number\n",
    "    '''\n",
    "    return(anchor_topic_numbers[assignment])\n",
    "#Get topic numbers from topic assignments\n",
    "assignment = list()\n",
    "for i in range(corpus.shape[0]):\n",
    "    assignment.append(twenty_train.target_names[twenty_train.target[i]])\n",
    "#Add true topic and topic number to dataset \n",
    "corpus['assignment'] = assignment\n",
    "corpus['topic_number'] = corpus['assignment'].apply(get_topic_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "comp.sys.ibm.pc.hardware:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "misc.forsale:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "rec.motorcycles:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "rec.sport.baseball:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "sci.crypt:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "sci.electronics:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "sci.med:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "sci.space:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "soc.religion.christian:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "talk.politics.guns:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n",
      "talk.politics.mideast:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011\n"
     ]
    }
   ],
   "source": [
    "#Display the topic words\n",
    "#number of topic words to see\n",
    "num_top_words=4\n",
    "#gathering topics from corex topic model extraction \n",
    "topics = topic_model.get_topics()\n",
    "\n",
    "#Iterate through the topics to print the words\n",
    "topic_words_number = list()\n",
    "for topic_n,topic in enumerate(topics):\n",
    "    #Get topic words and word parameters\n",
    "    words,mis = zip(*topic)\n",
    "    #sort by parameter strength\n",
    "    index = np.argsort(mis)\n",
    "    #get the index of top words, only if they are as \n",
    "    index = [i for i,j in zip(index,mis)]\n",
    "    #create string of topic words\n",
    "    topic_str = num_to_topic[topic_n]+':   '+', '.join(temp[:num_top_words])\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "           comp.graphics       0.71      0.66      0.68       584\n",
      "comp.sys.ibm.pc.hardware       0.64      0.78      0.70       590\n",
      "            misc.forsale       0.64      0.67      0.65       585\n",
      "         rec.motorcycles       0.43      0.90      0.58       598\n",
      "      rec.sport.baseball       0.91      0.82      0.86       597\n",
      "               sci.crypt       0.88      0.72      0.79       595\n",
      "         sci.electronics       0.84      0.46      0.59       591\n",
      "                 sci.med       0.84      0.70      0.76       594\n",
      "               sci.space       0.89      0.70      0.78       593\n",
      "  soc.religion.christian       0.85      0.90      0.87       599\n",
      "      talk.politics.guns       0.80      0.71      0.75       546\n",
      "   talk.politics.mideast       0.92      0.79      0.85       564\n",
      "\n",
      "             avg / total       0.78      0.73      0.74      7036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [i for i in anchor_topic_dict]\n",
    "target_names.append('noise')\n",
    "print(classification_report(corpus['topic_number'].values, \n",
    "                            corpus['topic_guess'].values,\n",
    "                            target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "           comp.graphics       0.71      0.66      0.68       584\n",
      "comp.sys.ibm.pc.hardware       0.64      0.78      0.70       590\n",
      "            misc.forsale       0.64      0.67      0.65       585\n",
      "         rec.motorcycles       0.43      0.90      0.58       598\n",
      "      rec.sport.baseball       0.91      0.82      0.86       597\n",
      "               sci.crypt       0.88      0.72      0.79       595\n",
      "         sci.electronics       0.84      0.46      0.59       591\n",
      "                 sci.med       0.84      0.70      0.76       594\n",
      "               sci.space       0.89      0.70      0.78       593\n",
      "  soc.religion.christian       0.85      0.90      0.87       599\n",
      "      talk.politics.guns       0.80      0.71      0.75       546\n",
      "   talk.politics.mideast       0.92      0.79      0.85       564\n",
      "\n",
      "             avg / total       0.78      0.73      0.74      7036\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics:   file:0.012, format:0.007, image:0.006, gif:0.005, graphic:0.004\n",
      "comp.sys.ibm.pc.hardware:   drive:0.017, scsi:0.013, mb:0.011, card:0.011, controller:0.010\n",
      "misc.forsale:   sale:0.008, offer:0.008, shipping:0.008, condition:0.004, manual:0.004\n",
      "rec.motorcycles:   bike:0.021, motorcycle:0.006, helmet:0.006, ride:0.006, dod:0.005\n",
      "rec.sport.baseball:   team:0.009, game:0.009, player:0.007, pitching:0.006, pitcher:0.005\n",
      "sci.crypt:   key:0.023, encryption:0.013, chip:0.009, privacy:0.004, security:0.004\n",
      "sci.electronics:   amp:0.003, dsl:0.009, chastity:0.009, jxp:0.009, shameful:0.009\n",
      "sci.med:   food:0.006, patient:0.006, disease:0.006, doctor:0.005, pain:0.003\n",
      "sci.space:   space:0.009, nasa:0.008, orbit:0.007, launch:0.005, moon:0.005\n",
      "soc.religion.christian:   christian:0.012, jesus:0.012, church:0.009, bible:0.008, christ:0.007\n",
      "talk.politics.guns:   criminal:0.004, militia:0.004, handgun:0.003, gun:0.009, gun_control:0.002\n",
      "talk.politics.mideast:   armenian:0.016, israel:0.015, israeli:0.012, arab:0.011, jew:0.008\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think subject generates contradictory advice traffic law enforcement everybody opinion dead certain yet information extremely difficult cop traffic school instructor vehicle code tell part story judge choose interpret law wide variety way public large seldom hear advice disagree experienced believe suggest copy vehicle code study sit day court happens read fight ticket miss little section end say chance lousy basically screwed guessed pretty system california carefully prepared court bringing witness revealing serious hole officer story maximum fine plus assessment message clear judge appreciate john public trying case advice find ticket traffic school serious matter lawyer lawyer present exact case difference sentence\n",
      "\n",
      "thank everyone took time respond post fighting ticket wrote successfully fought case court others lost due cop outright lying judge circumstance surrounding ticket fellow lost judge appear mood several suggested obtain book called fight ticket general theme prepared research possible review scene happened tape paper witness around list question ask cop judge positive innocent proven guilty suggested trying court date changed trip officer show date day feel know chance win individual stated officer expert witness say speeding damn speeding say radar gun lot suggestion idea happens big day \\\n",
      "\n",
      "currently jump boot issue military kind made sell either clothing sale store u price match store co sold somewhat cheaper design actually care wear long high boot called boot similar construction boot except boot shape strap wrap around nice boot\n",
      "\n",
      "lord god sky personal appearance board director meeting change policy odds equal extremely lucky ever\n",
      "\n",
      "faq\n",
      "\n",
      "park watch\n",
      "\n",
      "word seems willing wait money insurance save pay insurance insurance pay insurance co pack large lawyer recover damage insurance lawyer sad true call job security lawyer later\n",
      "\n",
      "request opinion piece aerostitch piece aerostitch looking pc pc protection\n",
      "\n",
      "paraphrase initial post fight speeding ticket court reply fight ticket california edition david brown st ed berkeley ca press edition library luck u go\n",
      "\n",
      "look attitude problem mr comment sensitivity drinking driving girlfriend killed asshole learn verbal abuse flame brain stay newsgroup\n",
      "\n",
      "article px kw acc virginia edu med virginia edu dance federal ranger ec tm\n",
      "\n",
      "exactly dangerous look hard little protection keeping trouble mean knowing limit keeping machine shape able predict stupid move driver deal fun staying alive take conscious effort\n",
      "\n",
      "brand top end chamber clean black paul contact email_address contact seller cheer\n",
      "\n",
      "yes unfortunately concept owner car responsible action authorized user car biggest argument photo radar system trouble recourse mi issued ticket error computer follow around city chicago informal city work issue dozen parking ticket foot tire city\n",
      "\n",
      "included sale cover cover sold separately trailer sold\n",
      "\n",
      "friend refer ground magnet\n",
      "\n",
      "yeah judgement reaction realise wish patience stop day pay\n",
      "\n",
      "drink drive pay smiley\n",
      "\n",
      "yup unfortunately pointed cost insurance fault driver bandit longer bear responsibility paying insurance price crappy driver driver pay nose spread cost crappy driver action fair plan cap rate crappy driver inherently piece shit rest u end paying plan us speeding ticket basis raising rate piece shit based upon lie faster driver inherently le safe slower driver year ago later\n",
      "\n",
      "geez happened ticket driving slow oh saying edu breath okay\n",
      "\n",
      "volvo come freeway land top pool win jim\n",
      "\n",
      "sale ann arbor michigan kawasaki ex mile cherry red excellent condition asking\n",
      "\n",
      "quite simple code week manufacture\n",
      "\n",
      "maybe rid buy rice rocket certainly explain unless maybe piece toilet paper stuck bottom boot rich\n",
      "\n",
      "perhaps referring wife child sitting next moto ever heard game called drinking game older\n",
      "\n",
      "hmmm security guard place parking sticker driver side window reflection cause accident suppose state stephen\n",
      "\n",
      "volvo driver\n",
      "\n",
      "mail archive location msf program ibm thanks\n",
      "\n",
      "local hospital favorite curse\n",
      "\n",
      "skin stick frozen bed pan apple juice mistakenly drawn lab\n",
      "\n",
      "sale kawasaki ex mile excellent condition kept garage asking\n",
      "\n",
      "mike johnson com\n",
      "\n",
      "drive talking\n",
      "\n",
      "thanks playing making turn couple foot ground michael\n",
      "\n",
      "looked manage find listing anybody vhs persuaded lend watch\n",
      "\n",
      "exactly eye change contact correct due strange shape cornea take surgery michael com accepted\n",
      "\n",
      "citizen duty force government accountability anecdote deleted keep mind cop lie court started asking decide fight ready devise strategy cop story judge jury mind\n",
      "\n",
      "wow course georgia cheaper list look add heavy boot work combat similar\n",
      "\n",
      "test\n",
      "\n",
      "rd ca jody writes already discussed mail jonathan film inspector general danny although quote name leading lady jonathan think earlier russian film movie tv say based yes jonathan looked story\n",
      "\n",
      "interesting fight ticket chance cop show secondly show point lied ticket beleive yo charged mph posted speed severe ticket\n",
      "\n",
      "sex life maybe\n",
      "\n",
      "edu breath brain\n",
      "\n",
      "unless insurance agent offer multi vehicle discount time car assuming capable progressive offer multi vehicle discount price imho tony\n",
      "\n",
      "sound pretty lame understand friend charge extra usually pay little seems serviced friendly sale\n",
      "\n",
      "thought posted woman came court three witness woman car neighbor heard shouting lawyer odds multiple complaint way judge history finding everyone guilty convinced u came lawyer drop everything net result bill court cost trouble together quite week beer money\n",
      "\n",
      "beleive ny state considered eliminating toll motor cycle based simply fact toll booth mario realized trading hundred relief traffic bad court justice job thought rid forever\n",
      "\n",
      "posted friend reply net bmw rt sale\n",
      "\n",
      "fourth edition probably high gotta list\n",
      "\n",
      "traffic citation accusation committed crime motion trial innocent proven guilty cop one accuse committing crime witness crime highway patrol explain situation description car license number tell specific violation law witnessed wish prosecute ie search vehicle code section number handy fill ticket sign system ticket cop writes go appear court prosecute word carry weight cop\n",
      "\n",
      "sale harley liberty edition condition extra asking located rhode island\n",
      "\n",
      "looking cx turbo trade duc\n",
      "\n",
      "true file complaint bring person court understand citizen arrest physical detention person\n",
      "\n",
      "citizen arrest felony\n",
      "\n",
      "yeah darn cover glass driver reasonably expect able drive thing car\n",
      "\n",
      "panel judge answer send copy gang faq\n",
      "\n",
      "choke cop gave ticket sounding speeding probably show court lawyer simply sounding speeding ridiculous found guilty appeal show lawyer cop car plane radar electronic speed measuring device pace within mph limit court accept cop seeing fast limit looked zone beyond reasonable doubt granted lose case measured eye court\n",
      "\n",
      "sound suspiciously fault advertised getting lawyer loop sigh naive illusion toilet legislator lawyer difficult law passed cut lawyer business fault insurance law always\n",
      "\n",
      "setup site arnie later\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ticket', 17),\n",
       " ('court', 14),\n",
       " ('cop', 12),\n",
       " ('lawyer', 12),\n",
       " ('driver', 11),\n",
       " ('insurance', 9),\n",
       " ('judge', 8),\n",
       " ('boot', 8),\n",
       " ('sale', 7),\n",
       " ('car', 7),\n",
       " ('fight', 6),\n",
       " ('speeding', 6),\n",
       " ('pay', 6),\n",
       " ('traffic', 5),\n",
       " ('law', 5),\n",
       " ('vehicle', 5),\n",
       " ('witness', 5),\n",
       " ('show', 5),\n",
       " ('piece', 5),\n",
       " ('code', 4),\n",
       " ('story', 4),\n",
       " ('day', 4),\n",
       " ('say', 4),\n",
       " ('case', 4),\n",
       " ('guilty', 4),\n",
       " ('u', 4),\n",
       " ('edition', 4),\n",
       " ('edu', 4),\n",
       " ('asking', 4),\n",
       " ('advice', 3),\n",
       " ('little', 3),\n",
       " ('end', 3),\n",
       " ('chance', 3),\n",
       " ('system', 3),\n",
       " ('officer', 3),\n",
       " ('called', 3),\n",
       " ('around', 3),\n",
       " ('list', 3),\n",
       " ('radar', 3),\n",
       " ('price', 3),\n",
       " ('sold', 3),\n",
       " ('shape', 3),\n",
       " ('later', 3),\n",
       " ('look', 3),\n",
       " ('trouble', 3),\n",
       " ('limit', 3),\n",
       " ('contact', 3),\n",
       " ('city', 3),\n",
       " ('cover', 3),\n",
       " ('friend', 3),\n",
       " ('drive', 3),\n",
       " ('cost', 3),\n",
       " ('fault', 3),\n",
       " ('crappy', 3),\n",
       " ('based', 3),\n",
       " ('condition', 3),\n",
       " ('maybe', 3),\n",
       " ('looked', 3),\n",
       " ('citizen', 3),\n",
       " ('jonathan', 3),\n",
       " ('posted', 3),\n",
       " ('crime', 3),\n",
       " ('think', 2),\n",
       " ('opinion', 2),\n",
       " ('extremely', 2),\n",
       " ('difficult', 2),\n",
       " ('school', 2),\n",
       " ('tell', 2),\n",
       " ('way', 2),\n",
       " ('public', 2),\n",
       " ('large', 2),\n",
       " ('copy', 2),\n",
       " ('happens', 2),\n",
       " ('section', 2),\n",
       " ('pretty', 2),\n",
       " ('california', 2),\n",
       " ('prepared', 2),\n",
       " ('serious', 2),\n",
       " ('trying', 2),\n",
       " ('find', 2),\n",
       " ('everyone', 2),\n",
       " ('time', 2),\n",
       " ('post', 2),\n",
       " ('lost', 2),\n",
       " ('due', 2),\n",
       " ('appear', 2),\n",
       " ('suggested', 2),\n",
       " ('general', 2),\n",
       " ('happened', 2),\n",
       " ('paper', 2),\n",
       " ('innocent', 2),\n",
       " ('proven', 2),\n",
       " ('date', 2),\n",
       " ('win', 2),\n",
       " ('issue', 2),\n",
       " ('store', 2),\n",
       " ('co', 2),\n",
       " ('cheaper', 2),\n",
       " ('high', 2),\n",
       " ('similar', 2),\n",
       " ('change', 2),\n",
       " ('odds', 2),\n",
       " ('ever', 2),\n",
       " ('faq', 2),\n",
       " ('watch', 2),\n",
       " ('word', 2),\n",
       " ('seems', 2),\n",
       " ('money', 2),\n",
       " ('true', 2),\n",
       " ('job', 2),\n",
       " ('security', 2),\n",
       " ('aerostitch', 2),\n",
       " ('looking', 2),\n",
       " ('pc', 2),\n",
       " ('protection', 2),\n",
       " ('reply', 2),\n",
       " ('ca', 2),\n",
       " ('go', 2),\n",
       " ('drinking', 2),\n",
       " ('driving', 2),\n",
       " ('brain', 2),\n",
       " ('virginia', 2),\n",
       " ('exactly', 2),\n",
       " ('keeping', 2),\n",
       " ('able', 2),\n",
       " ('take', 2),\n",
       " ('top', 2),\n",
       " ('yes', 2),\n",
       " ('unfortunately', 2),\n",
       " ('action', 2),\n",
       " ('work', 2),\n",
       " ('parking', 2),\n",
       " ('foot', 2),\n",
       " ('ground', 2),\n",
       " ('yeah', 2),\n",
       " ('wish', 2),\n",
       " ('paying', 2),\n",
       " ('plan', 2),\n",
       " ('rate', 2),\n",
       " ('inherently', 2),\n",
       " ('shit', 2),\n",
       " ('lie', 2),\n",
       " ('breath', 2),\n",
       " ('volvo', 2),\n",
       " ('kawasaki', 2),\n",
       " ('ex', 2),\n",
       " ('mile', 2),\n",
       " ('excellent', 2),\n",
       " ('quite', 2),\n",
       " ('week', 2),\n",
       " ('rid', 2),\n",
       " ('explain', 2),\n",
       " ('unless', 2),\n",
       " ('toilet', 2),\n",
       " ('heard', 2),\n",
       " ('game', 2),\n",
       " ('state', 2),\n",
       " ('mail', 2),\n",
       " ('thanks', 2),\n",
       " ('com', 2),\n",
       " ('michael', 2),\n",
       " ('eye', 2),\n",
       " ('mind', 2),\n",
       " ('writes', 2),\n",
       " ('film', 2),\n",
       " ('beleive', 2),\n",
       " ('mph', 2),\n",
       " ('speed', 2),\n",
       " ('offer', 2),\n",
       " ('multi', 2),\n",
       " ('discount', 2),\n",
       " ('sound', 2),\n",
       " ('understand', 2),\n",
       " ('extra', 2),\n",
       " ('thought', 2),\n",
       " ('woman', 2),\n",
       " ('came', 2),\n",
       " ('complaint', 2),\n",
       " ('net', 2),\n",
       " ('toll', 2),\n",
       " ('simply', 2),\n",
       " ('probably', 2),\n",
       " ('number', 2),\n",
       " ('prosecute', 2),\n",
       " ('person', 2),\n",
       " ('arrest', 2),\n",
       " ('sounding', 2),\n",
       " ('subject', 1),\n",
       " ('generates', 1),\n",
       " ('contradictory', 1),\n",
       " ('enforcement', 1),\n",
       " ('everybody', 1),\n",
       " ('dead', 1),\n",
       " ('certain', 1),\n",
       " ('yet', 1),\n",
       " ('information', 1),\n",
       " ('instructor', 1),\n",
       " ('part', 1),\n",
       " ('choose', 1),\n",
       " ('interpret', 1),\n",
       " ('wide', 1),\n",
       " ('variety', 1),\n",
       " ('seldom', 1),\n",
       " ('hear', 1),\n",
       " ('disagree', 1),\n",
       " ('experienced', 1),\n",
       " ('believe', 1),\n",
       " ('suggest', 1),\n",
       " ('study', 1),\n",
       " ('sit', 1),\n",
       " ('read', 1),\n",
       " ('miss', 1),\n",
       " ('lousy', 1),\n",
       " ('basically', 1),\n",
       " ('screwed', 1),\n",
       " ('guessed', 1),\n",
       " ('carefully', 1),\n",
       " ('bringing', 1),\n",
       " ('revealing', 1),\n",
       " ('hole', 1),\n",
       " ('maximum', 1),\n",
       " ('fine', 1),\n",
       " ('plus', 1),\n",
       " ('assessment', 1),\n",
       " ('message', 1),\n",
       " ('clear', 1),\n",
       " ('appreciate', 1),\n",
       " ('john', 1),\n",
       " ('matter', 1),\n",
       " ('present', 1),\n",
       " ('exact', 1),\n",
       " ('difference', 1),\n",
       " ('sentence', 1),\n",
       " ('thank', 1),\n",
       " ('took', 1),\n",
       " ('respond', 1),\n",
       " ('fighting', 1),\n",
       " ('wrote', 1),\n",
       " ('successfully', 1),\n",
       " ('fought', 1),\n",
       " ('others', 1),\n",
       " ('outright', 1),\n",
       " ('lying', 1),\n",
       " ('circumstance', 1),\n",
       " ('surrounding', 1),\n",
       " ('fellow', 1),\n",
       " ('mood', 1),\n",
       " ('several', 1),\n",
       " ('obtain', 1),\n",
       " ('book', 1),\n",
       " ('theme', 1),\n",
       " ('research', 1),\n",
       " ('possible', 1),\n",
       " ('review', 1),\n",
       " ('scene', 1),\n",
       " ('tape', 1),\n",
       " ('question', 1),\n",
       " ('ask', 1),\n",
       " ('positive', 1),\n",
       " ('changed', 1),\n",
       " ('trip', 1),\n",
       " ('feel', 1),\n",
       " ('know', 1),\n",
       " ('individual', 1),\n",
       " ('stated', 1),\n",
       " ('expert', 1),\n",
       " ('damn', 1),\n",
       " ('gun', 1),\n",
       " ('lot', 1),\n",
       " ('suggestion', 1),\n",
       " ('idea', 1),\n",
       " ('big', 1),\n",
       " ('\\\\', 1),\n",
       " ('currently', 1),\n",
       " ('jump', 1),\n",
       " ('military', 1),\n",
       " ('kind', 1),\n",
       " ('made', 1),\n",
       " ('sell', 1),\n",
       " ('either', 1),\n",
       " ('clothing', 1),\n",
       " ('match', 1),\n",
       " ('somewhat', 1),\n",
       " ('design', 1),\n",
       " ('actually', 1),\n",
       " ('care', 1),\n",
       " ('wear', 1),\n",
       " ('long', 1),\n",
       " ('construction', 1),\n",
       " ('except', 1),\n",
       " ('strap', 1),\n",
       " ('wrap', 1),\n",
       " ('nice', 1),\n",
       " ('lord', 1),\n",
       " ('god', 1),\n",
       " ('sky', 1),\n",
       " ('personal', 1),\n",
       " ('appearance', 1),\n",
       " ('board', 1),\n",
       " ('director', 1),\n",
       " ('meeting', 1),\n",
       " ('policy', 1),\n",
       " ('equal', 1),\n",
       " ('lucky', 1),\n",
       " ('park', 1),\n",
       " ('willing', 1),\n",
       " ('wait', 1),\n",
       " ('save', 1),\n",
       " ('pack', 1),\n",
       " ('recover', 1),\n",
       " ('damage', 1),\n",
       " ('sad', 1),\n",
       " ('call', 1),\n",
       " ('request', 1),\n",
       " ('paraphrase', 1),\n",
       " ('initial', 1),\n",
       " ('david', 1),\n",
       " ('brown', 1),\n",
       " ('st', 1),\n",
       " ('ed', 1),\n",
       " ('berkeley', 1),\n",
       " ('press', 1),\n",
       " ('library', 1),\n",
       " ('luck', 1),\n",
       " ('attitude', 1),\n",
       " ('problem', 1),\n",
       " ('mr', 1),\n",
       " ('comment', 1),\n",
       " ('sensitivity', 1),\n",
       " ('girlfriend', 1),\n",
       " ('killed', 1),\n",
       " ('asshole', 1),\n",
       " ('learn', 1),\n",
       " ('verbal', 1),\n",
       " ('abuse', 1),\n",
       " ('flame', 1),\n",
       " ('stay', 1),\n",
       " ('newsgroup', 1),\n",
       " ('article', 1),\n",
       " ('px', 1),\n",
       " ('kw', 1),\n",
       " ('acc', 1),\n",
       " ('med', 1),\n",
       " ('dance', 1),\n",
       " ('federal', 1),\n",
       " ('ranger', 1),\n",
       " ('ec', 1),\n",
       " ('tm', 1),\n",
       " ('dangerous', 1),\n",
       " ('hard', 1),\n",
       " ('mean', 1),\n",
       " ('knowing', 1),\n",
       " ('machine', 1),\n",
       " ('predict', 1),\n",
       " ('stupid', 1),\n",
       " ('move', 1),\n",
       " ('deal', 1),\n",
       " ('fun', 1),\n",
       " ('staying', 1),\n",
       " ('alive', 1),\n",
       " ('conscious', 1),\n",
       " ('effort', 1),\n",
       " ('brand', 1),\n",
       " ('chamber', 1),\n",
       " ('clean', 1),\n",
       " ('black', 1),\n",
       " ('paul', 1),\n",
       " ('email_address', 1),\n",
       " ('seller', 1),\n",
       " ('cheer', 1),\n",
       " ('concept', 1),\n",
       " ('owner', 1),\n",
       " ('responsible', 1),\n",
       " ('authorized', 1),\n",
       " ('user', 1),\n",
       " ('biggest', 1),\n",
       " ('argument', 1),\n",
       " ('photo', 1),\n",
       " ('recourse', 1),\n",
       " ('mi', 1),\n",
       " ('issued', 1),\n",
       " ('error', 1),\n",
       " ('computer', 1),\n",
       " ('follow', 1),\n",
       " ('chicago', 1),\n",
       " ('informal', 1),\n",
       " ('dozen', 1),\n",
       " ('tire', 1),\n",
       " ('included', 1),\n",
       " ('separately', 1),\n",
       " ('trailer', 1),\n",
       " ('refer', 1),\n",
       " ('magnet', 1),\n",
       " ('judgement', 1),\n",
       " ('reaction', 1),\n",
       " ('realise', 1),\n",
       " ('patience', 1),\n",
       " ('stop', 1),\n",
       " ('drink', 1),\n",
       " ('smiley', 1),\n",
       " ('yup', 1),\n",
       " ('pointed', 1),\n",
       " ('bandit', 1),\n",
       " ('longer', 1),\n",
       " ('bear', 1),\n",
       " ('responsibility', 1),\n",
       " ('nose', 1),\n",
       " ('spread', 1),\n",
       " ('fair', 1),\n",
       " ('cap', 1),\n",
       " ('rest', 1),\n",
       " ('us', 1),\n",
       " ('basis', 1),\n",
       " ('raising', 1),\n",
       " ('upon', 1),\n",
       " ('faster', 1),\n",
       " ('le', 1),\n",
       " ('safe', 1),\n",
       " ('slower', 1),\n",
       " ('year', 1),\n",
       " ('ago', 1),\n",
       " ('geez', 1),\n",
       " ('slow', 1),\n",
       " ('oh', 1),\n",
       " ('saying', 1),\n",
       " ('okay', 1),\n",
       " ('come', 1),\n",
       " ('freeway', 1),\n",
       " ('land', 1),\n",
       " ('pool', 1),\n",
       " ('jim', 1),\n",
       " ('ann', 1),\n",
       " ('arbor', 1),\n",
       " ('michigan', 1),\n",
       " ('cherry', 1),\n",
       " ('red', 1),\n",
       " ('simple', 1),\n",
       " ('manufacture', 1),\n",
       " ('buy', 1),\n",
       " ('rice', 1),\n",
       " ('rocket', 1),\n",
       " ('certainly', 1),\n",
       " ('stuck', 1),\n",
       " ('bottom', 1),\n",
       " ('rich', 1),\n",
       " ('perhaps', 1),\n",
       " ('referring', 1),\n",
       " ('wife', 1),\n",
       " ('child', 1),\n",
       " ('sitting', 1),\n",
       " ('next', 1),\n",
       " ('moto', 1),\n",
       " ('older', 1),\n",
       " ('hmmm', 1),\n",
       " ('guard', 1),\n",
       " ('place', 1),\n",
       " ('sticker', 1),\n",
       " ('side', 1),\n",
       " ('window', 1),\n",
       " ('reflection', 1),\n",
       " ('cause', 1),\n",
       " ('accident', 1),\n",
       " ('suppose', 1),\n",
       " ('stephen', 1),\n",
       " ('archive', 1),\n",
       " ('location', 1),\n",
       " ('msf', 1),\n",
       " ('program', 1),\n",
       " ('ibm', 1),\n",
       " ('local', 1),\n",
       " ('hospital', 1),\n",
       " ('favorite', 1),\n",
       " ('curse', 1),\n",
       " ('skin', 1),\n",
       " ('stick', 1),\n",
       " ('frozen', 1),\n",
       " ('bed', 1),\n",
       " ('pan', 1),\n",
       " ('apple', 1),\n",
       " ('juice', 1),\n",
       " ('mistakenly', 1),\n",
       " ('drawn', 1),\n",
       " ('lab', 1),\n",
       " ('kept', 1),\n",
       " ('garage', 1),\n",
       " ('mike', 1),\n",
       " ('johnson', 1),\n",
       " ('talking', 1),\n",
       " ('playing', 1),\n",
       " ('making', 1),\n",
       " ('turn', 1),\n",
       " ('couple', 1),\n",
       " ('manage', 1),\n",
       " ('listing', 1),\n",
       " ('anybody', 1),\n",
       " ('vhs', 1),\n",
       " ('persuaded', 1),\n",
       " ('lend', 1),\n",
       " ('correct', 1),\n",
       " ('strange', 1),\n",
       " ('cornea', 1),\n",
       " ('surgery', 1),\n",
       " ('accepted', 1),\n",
       " ('duty', 1),\n",
       " ('force', 1),\n",
       " ('government', 1),\n",
       " ('accountability', 1),\n",
       " ('anecdote', 1),\n",
       " ('deleted', 1),\n",
       " ('keep', 1),\n",
       " ('started', 1),\n",
       " ('decide', 1),\n",
       " ('ready', 1),\n",
       " ('devise', 1),\n",
       " ('strategy', 1),\n",
       " ('jury', 1),\n",
       " ('wow', 1),\n",
       " ('course', 1),\n",
       " ('georgia', 1),\n",
       " ('add', 1),\n",
       " ('heavy', 1),\n",
       " ('combat', 1),\n",
       " ('test', 1),\n",
       " ('rd', 1),\n",
       " ('jody', 1),\n",
       " ('already', 1),\n",
       " ('discussed', 1),\n",
       " ('inspector', 1),\n",
       " ('danny', 1),\n",
       " ('although', 1),\n",
       " ('quote', 1),\n",
       " ('name', 1),\n",
       " ('leading', 1),\n",
       " ('lady', 1),\n",
       " ('earlier', 1),\n",
       " ('russian', 1),\n",
       " ('movie', 1),\n",
       " ('tv', 1),\n",
       " ('interesting', 1),\n",
       " ('secondly', 1),\n",
       " ('point', 1),\n",
       " ('lied', 1),\n",
       " ('yo', 1),\n",
       " ('charged', 1),\n",
       " ('severe', 1),\n",
       " ('sex', 1),\n",
       " ('life', 1),\n",
       " ('agent', 1),\n",
       " ('assuming', 1),\n",
       " ('capable', 1),\n",
       " ('progressive', 1),\n",
       " ('imho', 1),\n",
       " ('tony', 1),\n",
       " ('lame', 1),\n",
       " ('charge', 1),\n",
       " ('usually', 1),\n",
       " ('serviced', 1),\n",
       " ('friendly', 1),\n",
       " ('three', 1),\n",
       " ('neighbor', 1),\n",
       " ('shouting', 1),\n",
       " ('multiple', 1),\n",
       " ('history', 1),\n",
       " ('finding', 1),\n",
       " ('convinced', 1),\n",
       " ('drop', 1),\n",
       " ('everything', 1),\n",
       " ('result', 1),\n",
       " ('bill', 1),\n",
       " ('together', 1),\n",
       " ('beer', 1),\n",
       " ('ny', 1),\n",
       " ('considered', 1),\n",
       " ('eliminating', 1),\n",
       " ('motor', 1),\n",
       " ('cycle', 1),\n",
       " ('fact', 1),\n",
       " ('booth', 1),\n",
       " ('mario', 1),\n",
       " ('realized', 1),\n",
       " ('trading', 1),\n",
       " ('hundred', 1),\n",
       " ('relief', 1),\n",
       " ('bad', 1),\n",
       " ('justice', 1),\n",
       " ('forever', 1),\n",
       " ('bmw', 1),\n",
       " ('rt', 1),\n",
       " ('fourth', 1),\n",
       " ('gotta', 1),\n",
       " ('citation', 1),\n",
       " ('accusation', 1),\n",
       " ('committed', 1),\n",
       " ('motion', 1),\n",
       " ('trial', 1),\n",
       " ('one', 1),\n",
       " ('accuse', 1),\n",
       " ('committing', 1),\n",
       " ('highway', 1),\n",
       " ('patrol', 1),\n",
       " ('situation', 1),\n",
       " ('description', 1),\n",
       " ('license', 1),\n",
       " ('specific', 1),\n",
       " ('violation', 1),\n",
       " ('witnessed', 1),\n",
       " ('ie', 1),\n",
       " ('search', 1),\n",
       " ('handy', 1),\n",
       " ('fill', 1),\n",
       " ('sign', 1),\n",
       " ('carry', 1),\n",
       " ('weight', 1),\n",
       " ('harley', 1),\n",
       " ('liberty', 1),\n",
       " ('located', 1),\n",
       " ('rhode', 1),\n",
       " ('island', 1),\n",
       " ('cx', 1),\n",
       " ('turbo', 1),\n",
       " ('trade', 1),\n",
       " ('duc', 1),\n",
       " ('file', 1),\n",
       " ('bring', 1),\n",
       " ('physical', 1),\n",
       " ('detention', 1),\n",
       " ('felony', 1),\n",
       " ('darn', 1),\n",
       " ('glass', 1),\n",
       " ('reasonably', 1),\n",
       " ('expect', 1),\n",
       " ('thing', 1),\n",
       " ('panel', 1),\n",
       " ('answer', 1),\n",
       " ('send', 1),\n",
       " ('gang', 1),\n",
       " ('choke', 1),\n",
       " ('gave', 1),\n",
       " ('ridiculous', 1),\n",
       " ('found', 1),\n",
       " ('appeal', 1),\n",
       " ('plane', 1),\n",
       " ('electronic', 1),\n",
       " ('measuring', 1),\n",
       " ('device', 1),\n",
       " ('pace', 1),\n",
       " ('within', 1),\n",
       " ('accept', 1),\n",
       " ('seeing', 1),\n",
       " ('fast', 1),\n",
       " ('zone', 1),\n",
       " ('beyond', 1),\n",
       " ('reasonable', 1),\n",
       " ('doubt', 1),\n",
       " ('granted', 1),\n",
       " ('lose', 1),\n",
       " ('measured', 1),\n",
       " ('suspiciously', 1),\n",
       " ('advertised', 1),\n",
       " ('getting', 1),\n",
       " ('loop', 1),\n",
       " ('sigh', 1),\n",
       " ('naive', 1),\n",
       " ('illusion', 1),\n",
       " ('legislator', 1),\n",
       " ('passed', 1),\n",
       " ('cut', 1),\n",
       " ('business', 1),\n",
       " ('always', 1),\n",
       " ('setup', 1),\n",
       " ('site', 1),\n",
       " ('arnie', 1)]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = list()\n",
    "count = 0\n",
    "topic_test = 'rec.motorcycles'\n",
    "for text,topic,guess in zip(corpus['final_text'],corpus['assignment'],corpus['topic_guess'] ):\n",
    "    if topic ==  topic_test and topic_test != num_to_topic[guess]:\n",
    "        temp.extend(text.split(' '))\n",
    "        count += 1\n",
    "        print(text)\n",
    "        print()\n",
    "\n",
    "Counter(temp).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
